---
title: "Untitled"
format: html
---


```{r}
#| label: setup
#| echo: FALSE
library(tidyr)
library(readr)
library(stringr)
library(tokenizers)
library(dplyr)
library(gt)
```


## Task 1: Data processing/cleaning  
### Sampling
To get a representation of data from each of the three sources, I chose to sample equal numbers of lines from each sample. This was straightforward to implement but glosses over that lines from each file might have different numbers of words; for this analysis I chose to disregard this factor. 

```{r, cache = TRUE}
set.seed(1234)

blogs_path <- "final/en_US/en_US.blogs.txt"
news_path <- "final/en_US/en_US.news.txt"
twitter_path <- "final/en_US/en_US.twitter.txt"

sample_corpus <- function(path, n = 100000) {
  file_length <- R.utils::countLines(con <- file(path, "rb")); close(con)
  data <- readLines(con <- file(path, "rb"))[sample(1:file_length, n, replace = FALSE)]; close(con)
  data
}

if (!file.exists("data/data.csv")) {
  data_blogs <- sample_corpus(blogs_path)
  data_news <- sample_corpus(news_path)
  data_twitter <- sample_corpus(twitter_path)
  data <- c(data_blogs, data_news, data_twitter)
  write_csv(data.frame(lines = data), "data/data.csv")
}

data <- read_csv("data/data.csv", show_col_types = FALSE)
```

### Cleaning data
Next I opted to remove all punctuation as I am not planning on running any sentiment analysis, but rather solely prediction of next words. By the same logic, I also chose to remove numeric characters - when predicting the next line in a message a specific number is not a useful item to predict. Finally, I changed all letters to lowercase to remove any ambiguity between phrases such as "i" and "I" or "pittsburgh" and "Pittsburgh.

```{r, cache = TRUE}
# Remove anything not [a-zA-Z], space, or apostrophe
data <- data$lines %>% gsub('[^[:alpha:][:space:]\']', '', .) %>%
  tolower()

data[1:5]
```

A potential shortcoming of this approach is that I am not using an interesting piece of information: beginning and end of sentence. In the structure of the English language, there are likely phrases that show up more often in the beginning, middle, or end of a sentence. Ultimately, for the model I am planning on building, simply predicting the next word, I am uncertain of the need for this and the additional overhead that the algorithm would run through. In a future implementation of this model, I would like to incorporate the beginning/end of sentence information.  

### Profanity Filtering
I searched for a list of profanity that included a wide assortment of negative words that I didn't want to include in the corpus. These were then removed from the data. 

```{r, cache = TRUE}
if (!file.exists("swearWords.txt")) {
  download.file("http://www.bannedwordlist.com/lists/swearWords.txt", "swearWords.txt")
}
swear_words <- read_csv("swearWords.txt", col_names = FALSE, show_col_types = FALSE)

word_match_list = function(...) {
  words = c(...)
  word_options = paste(words, collapse = "|") # combine the words w/ | between them
  paste0('\\b(?:', word_options, ')\\b') 
  # add extra regex formatting that makes it work
}

word_list_regex <-  word_match_list(swear_words$X1)
pre_cleanup_n <- sum(lengths(strsplit(data, "\\W+")))
data <- str_remove_all(data, word_list_regex)
post_cleanup_n <- sum(lengths(strsplit(data, "\\W+")))
```

Cleaning up with the chosen list of profanity removes a total of `r pre_cleanup_n - post_cleanup_n` words from the corpus. 

### Tokenize
Now that the data has been cleaned, I can tokenize, that is find instances of single words, bigrams (two word sets), trigrams (three word sets), and quadgrams (4 word sets). This is easily achieved with the `tokenize_ngrams` function where different lengths of ngrams are passed. 

```{r}
tokenized_words <- tokenize_words(data)
tokenized_2gram <- tokenize_ngrams(data, n = 2)
tokenized_3gram <- tokenize_ngrams(data, n = 3)
tokenized_4gram <- tokenize_ngrams(data, n = 4)
```

## Task 2: Exploratory Data Analysis
My next step in building the predictive model is to understand the distribution and relationship between words, tokens, and phrases of the text. To get a grasp of the material, I'll look at the distributions of 1, 2, and 3-grams as well as several questions detailed in the task I'll illustrate below. 

### Word Frequency
To begin with we can examine the distribution of words from the `tokenized_words` created above with `tokenizers::tokenize_words`. 

```{r}
word_data <- data.frame(words = unlist(tokenized_words))
most_common_words_0 <- word_data %>% 
  count(words, sort = TRUE)
```

Unsurprisingly, the words `r head(most_common_words_0$words)` are the most common but relatively uninteresting. These can be ignored as they are included in `tm::stopwords()`. 

```{r}
word_data_nsw <- word_data %>% 
  anti_join(data.frame(words = tm::stopwords("en")))
most_common_words_1 <- word_data_nsw %>%
  count(words, sort = TRUE)
```

By excluding stopwords, we can now make a plot of the most common words with ggplot. 

```{r}
library(ggplot2)
head(most_common_words_1, n = 8) %>% 
  mutate(words = factor(words, 
                        levels = head(most_common_words_1, n = 8)$words)) %>%
  ggplot(aes(x = words, y = n)) + 
  geom_col() + 
  labs(title = "10 most common words")
```


### 2-Gram Frequency
The single most common words don't tell us much information about the set, but common 2-grams may be more illuminating. 

```{r}
#| cache: true
#| dependson: most_common_bigrams
two_gram_data <- data.frame(two_gram = unlist(tokenized_2gram))

most_common_bigrams <- two_gram_data %>%
  count(two_gram, sort = TRUE)

gt(head(most_common_bigrams, n = 10))
```

### 3-Gram frequency
Similarly, three-grams may provide additional insight into common phrases. Initially I noticed that there were 11641 NA's that were the most prevalent type of trigram but realized these were due to finding trigrams of two word lines by evaluating `sum(sapply(tokenized_words, length) < 3)` and was rectified by passing the trigram data to `na.omit()`.

```{r}
#| cache: true
#| dependson: most_common_trigrams
trigram_data <- data.frame(trigram = unlist(tokenized_3gram)) %>%
  na.omit()

most_common_trigrams <- trigram_data %>%
  count(trigram, sort = TRUE)

gt(head(most_common_trigrams, n = 10))
```

### 4-gram frequency
For future analysis, I opted to create `most_common_quadgrams` in the same format as the prior work for `bigrams` and `trigrams`. 

```{r}
#| cache: true
#| dependson: most_common_quadgrams
quadgram_data <- data.frame(quadgram = unlist(tokenized_4gram)) %>%
  na.omit()

most_common_quadgrams <- quadgram_data %>%
  count(quadgram, sort = TRUE)

gt(head(most_common_quadgrams, n = 10))
```


### Words needed to cover dataset  
How many words are needed to cover 50% of all word instances? 90%? This is straight forward to answer if the words are sorted by their frequency, converted to proportion by dividing by total number of words in the dataset, and then aggregated with a cumulative sum. Similarly to the evaluation of most common words, for this question to provide an interesting answer, we can first filter out stopwords such as `the`, `and`, `a`, etc. 

```{r}
#| cache: TRUE
#| dependson: word_props
# word data, no stop words

word_props <- word_data_nsw %>%
  count(words, sort = TRUE) %>%
  mutate(prop = n/sum(n),
         cumprop = cumsum(prop)) %>%
  summarize(`50%` = with(., which.max(cumprop >= .5)), 
            `90%` = with(., which.max(cumprop >= .9)), 
            `100%` = nrow(.)) %>%
  pivot_longer(everything(), names_to = "Percent of corpus", values_to = "Number of words") %>% 
  select(c(2,1))

gt(word_props, caption = "Number of words to cover varying percentages of all word instances")
```

Though there are a total of `r nrow(unique(word_data_nsw))` words in the no-stop word dataset, it takes only `r round(word_props[[1,1]]/word_props[[3,1]]*100,2)`% of the total words to describe 50% of all word instances and `r round(word_props[[2,1]]/word_props[[3,1]]*100,2)`% of the total words to describe 90% of all word instances. If future implementation of an ngram model proves too slow, this is one place where a speedup could occur: if the size of the corpus is decreased evaluation may speed up. 

### Finding foreign language words  
If the data is obtained from websites of one primary language, then I would expect the majority of words acquired in the corpus to be all from the same language. What this indicates to me is that if you sort all word instances by their frequency, the high frequency words are likely to be from the target language while the low frequency words are likely to be typos, misspellings, or words from other languages.  
Looking at `tail(most_common_words_1, n = 10)`, we can see that there are assorted, predominantly foreign, characters clustered at the end of the sorted word list.  

```{r}
#| echo: FALSE
gt(tail(most_common_words_1, n = 10))
```

Alternatively, we could also look at which words contain non-ascii characters. A convenient function is `stringi::stri_enc_isascii()` which checks whether characters are encoded with 127 or less bits (length of ascii characters). I chose to include the foreign words in the corpus as there were relatively few and these basic filters would eliminate words like "café" and "cliché". 

## Next Steps  
### Prediction algorithm  
The next step in this project will be to create a prediction algorithm based on n-grams. A basic implementation would be to use the highest match of ngram if it exists to predict the next word via maximum likelihood. 

What I'm targeting to implement is a more rigorous treatment that includes linear Good-Turing smoothing (to help improve low count estimates) and Katz backoff. 

### Shiny app
The envisioned Shiny app will take a user's input and either upon click of a button or live will show the result of the next most likely word. Which implementation used will depend on how quickly the prediction algorithm runs: if a user has to wait more than 500ms or so for the result to automatically display they may wonder if anything is happening. Adding a button and perhaps a progress bar would show feedback that the algorithm is working behind the scenes. 'd like to work on predicting the next word based on the previous 1 word. The approach for this is based on Markov Chains where we can store bigrams in a data frame with three columns: 1 for the current word, 1 for the next word, and 1 for frequency. This is essentially constructing a lookup table where given a first word, we can look at the probable next word for the sentence. 

This structure is easily expanded to trigrams and quadgrams that can be used to predict on the prior 2 and 3 words. 

```{r}
monogram <- word_data %>%
  count(words, sort = TRUE)

bigram_sep <- most_common_bigrams %>%
  separate_wider_delim(cols = bigram, 
                       delim = " ", 
                       names = "first", "second")
trigram_sep <- most_common_trigrams %>% 
  #filter(n > 2) %>%
  separate_wider_delim(cols = trigram, 
                       delim = " ", 
                       names = c("first", "second", "third"))

quadgram_sep <- most_common_quadgrams %>% 
  #filter(n > 2) %>%
  separate_wider_delim(cols = quadgram, 
                       delim = " ", 
                       names = c("first", "second", "third", "fourth"))

##### OLD - consider deleting
predict_bigram <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase)
  last_word <- word(phrase, -1)
  bigram_sep[which(bigram_sep$first == last_word), ]$second[1:3]
}

predict_trigram <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  next_to_last <- word(phrase, -2)
  last_word <- word(phrase, -1)
  trigram_sep[which((trigram_sep$first == next_to_last) & 
                      (trigram_sep$second == last_word)), ]$third[1:3]
}

predict_quadgram <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  first <- word(phrase, -3)
  second <- word(phrase, -2)
  third <- word(phrase, -1)
  quadgram_sep[which((quadgram_sep$first == first) & 
                       (quadgram_sep$second == second) & 
                       (quadgram_sep$third == third)), ]$fourth[1:3]
}
#########
```

Each of these functions takes a phrase, strips off an ending whitespace (an error I encountered), and returns the top three words to come next. With longer ngrams, a common error with a smaller corpus is that a novel phrase may not be seen. To avoid the function returning nothing, we can build a function that takes a phrase and if a match isn't found, then use the next shorter phrase. For example, if "the quick brown" doesn't return a match, we could try to match "quick brown", and even "brown". 
```{r}
predict_gram <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  rtrn_guess <- predict_quadgram(phrase)
  if(is.na(rtrn_guess[2])) {
    rtrn_guess <- predict_trigram(phrase)
    if (is.na(rtrn_guess[2])) {
      rtrn_guess <- predict_bigram(phrase)
    }
  }
  rtrn_guess
}
```

#### Analysis  
This current iteration of the model only takes into account the frequency of a given phrase and doesn't examine the conditional probability of a particular phrase occurring. This approach is examined next.  

## N-Gram model part 2  
For a more robust model, we should really consider the conditional probability of certain phrases occurring rather than just the frequency of a particular n-gram. As discussed in [Speech and Language Processing: Chapter 3 N-Gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf), what we're really interested in is probability of word *w* given history *h* but doing this directly is difficult so we need clever ways to approximate it. Note the text uses some conventions for notation: 
* $P(X_i = "the")$, the probability of word word "the" is abbreviated as $P(the)$. 
* The sequence of *n* words is abbreviated as either $w_1 \dots w_n$ or $w_{1:n}$ so the expression $w_{1:n-1}$ means the string $w_1, w_2, \dots, w_{n-1}$
* For joint probability of each word in sequence having a particular value $P(X_1 = w_1, X_2 = w_2, \dots, X_n = w_n)$ we'll use $P(w_1, w_2, \dots, w_n)$

To find probability of an entire sequence of words, we can decompose $P(w_1, w_2, \dots, w_n)$ using the **chain rule of probability** to: 

$$
\begin{aligned}
P(w_{1:n}) &= P(w_1)P(w_2|w_1)P(w_3|w_{1:2})\dots P(w_n|w_{1:n-1}) \\
&=\Pi_{k=1}^n P(w_k|w_{1:k-1})
\end{aligned}
$$

This chain rule links computing joint probability of a sequence and computing conditional probability of a word given previous words. The intuition of an n-gram model is that instead of needing to compute the probability of a word given its entire history, we can **approximate** the history by using the last few words.  
Instead of computing $P(the|Walden Pond's water is so transparent that)$, we can approximate it with $P(the|that)$, the bigram. This can be extended to n-grams as $P(w_n|w_{n-1}) \approx P(w_n|w_{n-N+1:n-1})$, that is looking from (N-1) to -1 words in the past. The challenge of estimating these n-gram probabilities is with **MLE** which says:  
$$
P(w_n|w_{n-N+1:n-1}) = \frac{C(w_{n-N+1:n-1}w_n)}{C(w_{n-N+1:n-1})}
$$

This equation estimates the n-gram probability by dividing the observed frequency of a particular sequence by the observed frequency of the prefix. 

```{r}
predict_quadgram <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  first <- word(phrase, -3)
  second <- word(phrase, -2)
  third <- word(phrase, -1)
  # sum all counts of first three words of phrase
  whole_phrase <- sum(quadgram_sep[which((quadgram_sep$first == first) & 
                                       (quadgram_sep$second == second) & 
                                       (quadgram_sep$third == third)), ]$n)
  # number of instances of top quadgram
  top_phrase <- quadgram_sep[which((quadgram_sep$first == first) & 
                                       (quadgram_sep$second == second) & 
                                       (quadgram_sep$third == third)), ][1,]
  c(top_phrase$fourth, top_phrase$n/whole_phrase)
}
```

### Simple Good-Turing discount
Allows for unseen ngrams to be appropriately included in the dataset. The probability of an unseen ngram is assigned to be equal to the probability of rare ngrams ngrams with a count of 1. "A useful part of Good-Turing methodology is the estimate that the total probability of all unseen objects is N1 / N."

Main steps of simple Good-Turing: 
1. To handle the case that most of the $N_r$ are zero for large $r$, which is a necessary value when calculating $r^*$. Instead of depending on the raw count $N_r$ of each non-zero frequency, these non-zero values are averaged and replaced by $Z_r = \frac{N_r}{.5(t-q)}$ where $q,r,t$ are successive non-zero $N_r$. When $r$ is the first index, $q$ is taken as 0 and when $r$ is the last index, let $t = 2r-q$. From now on, most of the estimated frequencies will be based on $Z_c$ rather than $N_r$, even though we may still refer to $N_r$. 
2. Because both the ranges of $r$ and $Z_r$ are large, both the values of $r$ and $Z_r$ are mapped to log space. 
3. Fit a straight line by linear regression: $\log(Z_r) = a+b\log(r)$ to smooth $Z_r$. 

```{r}
LGT <- function(ngram_sep) {
  t1 <- table(ngram_sep$n) # Find frequency of different counts
  t <- c(as.numeric(names(t1))[-1],as.numeric(names(t1))[length(t1)]) # lead 1
  q <- c(0,as.numeric(names(t1))[-length(t1)]) # lag 1
  zr <- t1 / (.5*(t-q)) # calculate zr
  # convert from table object to dataframe
  zr <- as.data.frame(zr) %>%
    mutate(z_r = as.numeric(as.character(Var1))
           ,.keep = "unused") %>%
    select(c(2,1))
  
  # convert nr, unaltered data to dataframe
  nr <- as.data.frame(t1) %>% 
    mutate(n_r = as.numeric(as.character(Var1)), 
           .keep = "unused")
  smooth_zr <- lm(log10(Freq) ~ log10(z_r), data = zr)
  return(smooth_zr$coefficients)
}
```

```{r}
#| eval: FALSE
#| echo: FALSE

# The following plots the nr and zr values along with the smoothed line to verify that the data here is following the trajectory lined out in "Good-Turing Smoothing without Tears"
quadgram_sep <- read_csv("data/quadgram.csv")
t1 <- table(quadgram_sep$n) # Find frequency of different counts
t <- c(as.numeric(names(t1))[-1],as.numeric(names(t1))[length(t1)]) # lead 1
q <- c(0,as.numeric(names(t1))[-length(t1)]) # lag 1

zr <- t1 / (.5*(t-q)) # calculate zr
# convert from table object to dataframe
zr <- as.data.frame(zr) %>%
  mutate(z_r = as.numeric(as.character(Var1))
         ,.keep = "unused") %>%
  select(c(2,1))

# convert nr, unaltered data to dataframe
nr <- as.data.frame(t1) %>% 
  mutate(n_r = as.numeric(as.character(Var1)), 
         .keep = "unused")

# Find smoothing equation
smooth_zr <- lm(log10(Freq) ~ log10(z_r), data = zr)

library(ggplot2)
p1 <- ggplot(nr, aes(x = n_r, Freq)) + 
    geom_point() + 
    labs(x = "r, frequency", 
         y = "Nr, frequency of frequency") + 
    scale_x_continuous(trans = 'log10', 
                       breaks = 10^c(0:3)) + 
    scale_y_continuous(trans = 'log10', 
                       breaks = 10^c(-3:7)) + 
  coord_fixed(ratio = .5, 
              xlim = c(1, 700), 
              ylim = c(.01, 10^7))

p2 <- ggplot(zr, aes(x = z_r, Freq)) + 
    geom_point() + 
    labs(x = "r, frequency", 
         y = "Zr, transformed frequency of frequency") + 
    scale_x_continuous(trans = 'log10', 
                       breaks = 10^c(0:3)) + 
    scale_y_continuous(trans = 'log10', 
                       breaks = 10^c(-3:7)) + 
  coord_fixed(ratio = .5, 
              xlim = c(1, 700), 
              ylim = c(.01, 10^7)) + 
  geom_smooth(method = "lm", formula = y ~ x)

gridExtra::grid.arrange(p1, p2, nrow = 1)
```


To find $r^* = (r+1) \frac{S(N_{r+1})}{S(N_r)}$ we can use 

$$
\log(N_r) = a + b \log(r) \\
N_r = A r^b \\
$$
and 
$$
r^* = (r+1) \frac{N_{r+1}}{N_r} \\
 = (r+1) \frac{A(r+1)^b}{Ar^b} \\
 = r (1 + 1/r)^{b+1}
$$
Where instead of using the values $N_r$, we use the estimates of $Z_r$. The original formula for $r^*$ is termed the *Turing* estimator, and is typically reasonable for small $r$. When the process of replacing $N_r$ with smoothed values $S(N_r)$ we rename the estimator a *Good-Turing* estimator. The rule for choosing the Turing estimate over the Good-Turing estimate is to use Turing as long as they are significantly different from the Good-Turing estimate. The paper "Good-Turing Smoothing Without Tears" describes this point to be when the difference between Turing and Good-Turing is less than 1.65 times the standard deviation of the Turing estimate. Once you switch for the first time, you use Good-Turing for the remainder of the points. 

```{r}
# Helper: Takes in r-values (such as nr$n_r), and coefficients from smoothed fit of Good-Turing and returns estimates for r_star
rs_GT_est <- function(r, coef) {
  return(r * (1 + 1/r) ^ (1 + coef[2]))
}

# Helper: Turing estimates of r_star on nr, the raw counts
rs_T_est <- function(nr) {
  (nr$n_r + 1) * c(nr$Freq[-1],0) / nr$Freq
}

# Function to add Turing/Good-Turing discounting to existing ngram_sep data. 
add_GT_prob <- function(ngram_sep) {
  t1 <- table(ngram_sep$n) # Find frequency of different counts
  # convert nr, unaltered data to dataframe
  nr <- as.data.frame(t1) %>% 
    mutate(n_r = as.numeric(as.character(Var1)), 
           .keep = "unused")
  smooth_zr <- LGT(ngram_sep)
  # Compute Turing rstars
  rs_T <- rs_T_est(nr)
  # Compute Good-Turing rstars
  rs_GT <- rs_GT_est(nr$n_r, smooth_zr)
  
  # Compute the standard deviation of the Turing estimates as described in paper
  rs_T_sd <- rep(1, nrow(nr))
  for (i in 1:nrow(nr)) {
    rs_T_sd[i] <- (nr$n_r[i]+1)/nr$Freq[i] * sqrt(nr$Freq[i+1]*(1+nr$Freq[i+1]/nr$Freq[i]))
  }
  
  # Make switch from Turing to Linear Good-Turing estimates 
  rs_combined <- rep(0, nrow(nr))
  useturing <- TRUE
  for (i in 1:nrow(nr)) {
    if (!useturing) {
      rs_combined[i] <- rs_GT[i]
    } else if (abs(rs_T - rs_GT)[i] > 1.65*rs_T_sd[i]) {
      rs_combined[i] <- rs_T[i]
    } else {
      useturing <- FALSE
      rs_combined[i] <- rs_GT[i]
    }
  }
  
  ## renormalize probabilities 
  xN <- sum(nr$n_r*nr$Freq)
  sum_prob <- sum(rs_combined*nr$Freq/xN)
  # To renormalize, we want to include room for N0 which is approximated as N1/N
  rs_combined_norm <- (1-nr$Freq[1]/xN)*(rs_combined/xN) / sum_prob
  
  pr_freq_lookup <- data.frame(pr = rs_combined_norm, 
                               n = nr$n_r, 
                               n2 = rs_combined)
  
  left_join(ngram_sep, pr_freq_lookup, by = "n")
}

quadgram_sep <- add_GT_prob(quadgram_sep)
trigram_sep <- add_GT_prob(trigram_sep)
bigram_sep <- add_GT_prob(bigram_sep)
```

The following functions modify the original `predict_ngram` functions to instead use the modified Good-Turing probability estimates. This will allow the final implementation of a Katz backoff model. 

```{r}
predict_bigram_gt <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase)
  first <- word(phrase, -1)
  # sum all counts of first word of phrase
  whole_phrase <- sum(bigram_sep[which(bigram_sep$first == first), ]$pr)
  # number of instances of top bigram
  top_phrase <- bigram_sep[which(bigram_sep$first == first), ][1,]
  data.frame(prediction = top_phrase$second, prob = top_phrase$pr/whole_phrase)
}

predict_trigram_gt <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  first <- word(phrase, -2)
  second <- word(phrase, -1)
  whole_phrase <- sum(trigram_sep[which((trigram_sep$first == first) & 
                                          (trigram_sep$second == second)), ]$pr)
  top_phrase <- trigram_sep[which((trigram_sep$first == first) & 
                                    (trigram_sep$second == second)), ][1,]
  data.frame(prediction = top_phrase$third, prob = top_phrase$pr/whole_phrase)
}

predict_quadgram_gt <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  first <- word(phrase, -3)
  second <- word(phrase, -2)
  third <- word(phrase, -1)
  # sum all counts of first three words of phrase
  whole_phrase <- sum(quadgram_sep[which((quadgram_sep$first == first) & 
                                       (quadgram_sep$second == second) & 
                                       (quadgram_sep$third == third)), ]$pr)
  # number of instances of top quadgram
  top_phrase <- quadgram_sep[which((quadgram_sep$first == first) & 
                                       (quadgram_sep$second == second) & 
                                       (quadgram_sep$third == third)), ][1,]
  data.frame(prediction = top_phrase$fourth, prob = top_phrase$pr/whole_phrase)
}
```

#### Katz Backoff  
I have been misunderstanding the purpose of Katz backoff, much to my chagrin. The prior work has been assuming that what I'm trying to estimate is when I haven't seen a particular ngram *at all*. That is I'm trying to predict "gibberish sell the ___ " but I don't have "gibberish sell the" in my quadgram so I need to find the probability associated with "sell the ___ " instead. This isn't what Katz is for. Instead, it is trying to answer the question is "sell the house" or "sell the book" a better estimate based on the frequency of *both* the trigrams and the bigrams "the house" and "the book". If for instance "sell the house" shows up 10 times and "sell the book" 1 time, my current implementation would predict "house". However, if in the bigram set, "the house" is seen 12 times but "the book" is seen 10,000 time it makes sense that perhaps "sell the book" could be a better estimate. 

Start with bigram: we have two sets for a combination: $A(w_{i-1}) = {w: \text{Count}(w_{i-1}, w) > 0}$ and $B(w_{i-1}) = {w: \text{Count}(w_{i-1}, w) = 0}$

```{r}
katz_bigram <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  first <- word(phrase, -1) # find last word of phrase
  matches <- bigram_sep[which(bigram_sep$first == first),] #find next word 
  alpha <- 1 - sum(matches$n2) / sum(matches$n)
  top_match <- matches[1,]
  top_A_prob <- sum(matches$n) # total probability of all i-1
  # number of instances of top bigram
  top_A <- data.frame(prediction = top_match$second, prob = top_match$n2/top_A_prob)
  
  if(is.na(top_A[1])) {  # If no matches found, return NA
    return(NA)
  }
  
  # find set B: words that are not in A
  setB <- anti_join(monogram, bigram_sep[which(bigram_sep$first == phrase),], by = join_by(words == second))
  # Find top match in B and probability = alpha * c(wi)/c(w)
  top_B <- data.frame(prediction = setB$words[1], prob = alpha * setB$n[1]/sum(setB$n))
  if(top_A$prob > top_B$prob) {
    return(top_A)
  } else {
    return(top_B)
  }
}

katz_trigram <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  first <- word(phrase, -2)
  second <- word(phrase, -1)
  matches <- trigram_sep[which(trigram_sep$first == first & 
                                 trigram_sep$second == second),] #find next word 
  alpha <- 1 - sum(matches$n2) / sum(matches$n)
  
  top_match <- matches[1,]
  top_A_prob <- sum(matches$n) # total probability of all i-1
  # number of instances of top trigram
  top_A <- data.frame(prediction = top_match$third, prob = top_match$n2/top_A_prob)
  
  if(is.na(top_A[1])) { # If no matches found, return NA
    return(NA)
  }
  
  # find set B: words that are not in A
  setB <- anti_join(monogram, matches, by = join_by(words == third))
  # Find top match in B and probability = alpha * qBO(wi|wi-1) / sum(qBO(w|wi-1))
  
  setB_bigrams <- paste(second, setB$words)
  setB_matches <- most_common_bigrams[which(most_common_bigrams$two_gram %in% setB_bigrams),]
  top_B <- data.frame(prediction = word(setB_matches$two_gram[1], -1), 
                      prob =  alpha * setB_matches$n[1] / sum(setB_matches$n))
  

  if(top_A$prob > top_B$prob) {
    return(top_A)
  } else {
    return(top_B)
  }
}

katz_quadgram <- function(phrase) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  first <- word(phrase, -3)
  second <- word(phrase, -2)
  third <- word(phrase, -1)
  
  matches <- quadgram_sep[which(quadgram_sep$first == first & 
                                 quadgram_sep$second == second & 
                                  quadgram_sep$third == third),] #find next word 
  alpha <- 1 - sum(matches$n2) / sum(matches$n)
  top_match <- matches[1,]
  top_A_prob <- sum(matches$n) # total probability of all i-1
  # number of instances of top quadgram
  top_A <- data.frame(prediction = top_match$fourth, prob = top_match$n2/top_A_prob)
  
  if(is.na(top_A[1])) { # If no matches found, return NA
    return(NA)
  }
  
   # find set B: words that are not in A
  setB <- anti_join(monogram, matches, by = join_by(words == fourth))
  # Find top match in B and probability = alpha * qBO(wi|wi-2,wi-1) / sum(qBO(w|wi-2,wi-1))
  
  setB_trigrams <- paste(second, third, setB$words)
  
  setB_matches <- most_common_trigrams[which(most_common_trigrams$trigram %in% setB_trigrams),]
  top_B <- data.frame(prediction = word(setB_matches$trigram[1], -1), 
                      prob =  ifelse(nrow(setB_matches[1])==0, 0, alpha * setB_matches$n[1] / sum(setB_matches$n)))
  
  if(top_A$prob > top_B$prob) {
    return(top_A)
  } else {
    return(top_B)
  }
}

```

Final Prediction model
```{r}
katz_ngram <- function(phrase = character()) {
  phrase <- gsub(" $", "", phrase) # get rid of trailing whitespace
  len <- length(strsplit(phrase, "\\W+")[[1]])
  
  if (len >= 3) {
    rtrn_guess <- katz_quadgram(phrase)
    if(is.na(rtrn_guess[1])) {
      rtrn_guess <- katz_trigram(phrase)
      if (is.na(rtrn_guess[1])) {
        rtrn_guess <- katz_bigram(phrase)
        if(is.na(rtrn_guess[1])) {
          return("Unk")
        }
      }
    }
    rtrn_guess
  } else if (len == 2) {
    rtrn_guess <- katz_trigram(phrase)
    if(is.na(rtrn_guess[1])) {
      rtrn_guess <- katz_bigram(phrase)
      if (is.na(rtrn_guess[1])) {
        return("Unk")
      }
    } 
    rtrn_guess
  } else {
    rtrn_guess <- katz_bigram(phrase)
    if(is.na(rtrn_guess[1])) {
      return("Unk")
    }
    rtrn_guess
  }
}
```

## Benchmarking
If the `katz_ngram` is run with data from the full dataset, benchmarking shows that it takes 1.44s to run while if the data is pruned to observations seen more than 5 times, the computation time is reduced to .26s. 

`rbenchmark::benchmark(katz = katz_ngram("the end of"), replications = 100)`

## Ideas to improve
* Use `iconv("anaïs", to='ASCII//TRANSLIT')` to translate non-ascii characters. 

## Shiny app
To develop the shiny app, I'm concerned that I'll have to reduce the size of the datasets that the algorithm uses in it's computation. This will decrease the accuracy of the prediction but may be unavoidable to implement the shiny app. 